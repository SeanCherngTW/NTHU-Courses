{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_collocation():\n",
    "    collocation = set()\n",
    "    for line in open('bnc.coll.small.txt'):\n",
    "        start, end, distance, count = line.split('\\t')\n",
    "        collocation.add(start + ' ' + end + ' ' + distance)\n",
    "    return collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collocation = read_collocation()\n",
    "PRONS = set([line.strip('\\n') for line in open('prons.txt')])\n",
    "with open('HiFreWords') as f:\n",
    "    HiFreWords = set(f.readline().split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13511 13 17000\n"
     ]
    }
   ],
   "source": [
    "print(len(collocation), len(PRONS), len(HiFreWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens(str1):\n",
    "    return re.findall('[a-z]+', str1.lower())\n",
    "\n",
    "def ngrams(sent, n):\n",
    "    return [' '.join(x) for x in zip(*[sent[i:] for i in range(n) if i <= len(sent)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isCollocation(start, end, d):\n",
    "    global collocation\n",
    "    if start + ' ' + end + ' ' + str(d) in collocation:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Mapper():\n",
    "    sentsNGrams = set()\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        sent = tokens(line)\n",
    "        sentLen = len(sent)\n",
    "        \n",
    "        if sentLen < 10 or sentLen > 25:\n",
    "            continue\n",
    "            \n",
    "        for n in range(2, 6):\n",
    "            for ngram in ngrams(sent, n):\n",
    "                sentsNGrams.add(ngram)\n",
    "                \n",
    "        for ngram in sentsNGrams:\n",
    "            terms = ngram.split()\n",
    "            start = terms[0]\n",
    "            end = terms[-1]\n",
    "            n = len(terms)\n",
    "\n",
    "            if isCollocation(start, end, n-1):\n",
    "                print('%s %s %d\\t%s' % (start, end, n-1, line))\n",
    "            if isCollocation(end, start, 1-n):\n",
    "                print('%s %s %d\\t%s' % (end, start, 1-n, line))\n",
    "                \n",
    "        sentsNGrams.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Mapper_testing():\n",
    "    print('---------- MAPPER START ----------')\n",
    "    counter = 0\n",
    "    sentsNGrams = set()\n",
    "    mapperOutput = []\n",
    "\n",
    "    for line in open('bnc.sents.txt'):\n",
    "        sent = tokens(line)\n",
    "        sentLen = len(sent)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 20000 == 0:\n",
    "            print('MAPPER: Processing sentences ... %3d%%' % (counter / 2000))\n",
    "        \n",
    "        if sentLen < 10 or sentLen > 25:\n",
    "            continue\n",
    "            \n",
    "        for n in range(2, 6):\n",
    "            for ngram in ngrams(sent, n):\n",
    "                sentsNGrams.add(ngram)\n",
    "\n",
    "        for ngram in sentsNGrams:\n",
    "            terms = ngram.split()\n",
    "            start = terms[0]\n",
    "            end = terms[-1]\n",
    "            n = len(terms)\n",
    "\n",
    "            if isCollocation(start, end, n-1):\n",
    "                mapperOutput.append('%s %s %d\\t%s' % (start, end, n-1, line))\n",
    "            if isCollocation(end, start, 1-n):\n",
    "                mapperOutput.append('%s %s %d\\t%s' % (end, start, 1-n, line))\n",
    "                \n",
    "        sentsNGrams.clear()\n",
    "            \n",
    "    print('MAPPER: Output length = %d' % len(mapperOutput))\n",
    "    print('---------- MAPPER END ----------')\n",
    "\n",
    "    return sorted(mapperOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Reducer():\n",
    "    lineList = []\n",
    "    prev_key = ''\n",
    "    isFirst = True\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        if not isFirst:\n",
    "            current_key = line.split('\\t')[0]\n",
    "            if line.split('\\t')[0] == prev_key:\n",
    "                lineList.append(line)\n",
    "                prev_key = current_key\n",
    "                isFirst = False\n",
    "                continue\n",
    "        else:\n",
    "            isFirst = False\n",
    "            prev_key = line.split('\\t')[0]\n",
    "            lineList.append(line)\n",
    "            continue\n",
    "            \n",
    "        # All sentences in one group are read  \n",
    "        max_score = -999\n",
    "        max_line = ''\n",
    "        for l in lineList:\n",
    "            ngm, sent = l.split('\\t')\n",
    "            word, col, dist = ngm.split()\n",
    "            score = computeScore(word, sent)\n",
    "            if score > max_score:\n",
    "                max_line = l\n",
    "                max_score = score\n",
    "\n",
    "        ngm, sent = max_line.split('\\t')\n",
    "        word, col, dist = ngm.split()\n",
    "        print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
    "\n",
    "        prev_key = current_key\n",
    "        lineList.clear()\n",
    "        lineList.append(line)\n",
    "        \n",
    "    if lineList:\n",
    "        max_score = -999\n",
    "        max_line = ''\n",
    "        for l in lineList:\n",
    "            ngm, sent = l.split('\\t')\n",
    "            word, col, dist = ngm.split()\n",
    "            score = computeScore(word, sent)\n",
    "            if score > max_score:\n",
    "                max_line = l\n",
    "                max_score = score\n",
    "    \n",
    "        ngm, sent = max_line.split('\\t')\n",
    "        word, col, dist = ngm.split()\n",
    "        print('%s %s %s\\t%s' % (word, col, dist, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Reducer_testing(mapperOutput):\n",
    "    print('---------- REDUCER START ----------')\n",
    "\n",
    "    lineList = []\n",
    "    reducerOutput = []\n",
    "    prev_key = ''\n",
    "    isFirst = True\n",
    "    counter = 0\n",
    "    mapperOutputCount = len(mapperOutput)\n",
    "    \n",
    "    print('REDUCER: Start processing the best sentence (data size = %d)' % mapperOutputCount)\n",
    "\n",
    "    for line in mapperOutput:\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % (mapperOutputCount / 100) == 0:\n",
    "            print('REDUCER: Processing the best sentence ... %3d%%' % (counter * 100 / mapperOutputCount))\n",
    "        \n",
    "#         print('prev_key = %s' % prev_key)\n",
    "#         print('curr_key = %s' % line.split('\\t')[0])\n",
    "#         print(lineList)\n",
    "#         print('--------')\n",
    "        \n",
    "        if not isFirst:\n",
    "            current_key = line.split('\\t')[0]\n",
    "            if line.split('\\t')[0] == prev_key:\n",
    "                lineList.append(line)\n",
    "                prev_key = current_key\n",
    "                isFirst = False\n",
    "                continue\n",
    "        else:\n",
    "            isFirst = False\n",
    "            prev_key = line.split('\\t')[0]\n",
    "            lineList.append(line)\n",
    "            continue\n",
    "            \n",
    "        # All sentences in one group are read  \n",
    "        max_score = -999\n",
    "        max_line = ''\n",
    "        for l in lineList:\n",
    "            ngm, sent = l.split('\\t')\n",
    "            word, col, dist = ngm.split()\n",
    "            score = computeScore(word, sent)\n",
    "#             print(score, max_score, l)\n",
    "            if score > max_score:\n",
    "                max_line = l\n",
    "                max_score = score\n",
    "        \n",
    "        ngm, sent = max_line.split('\\t')\n",
    "        word, col, dist = ngm.split()\n",
    "#         print('BEST SENT:')\n",
    "#         print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
    "#         print()\n",
    "        reducerOutput.append('%s %s %s\\t%s' % (word, col, dist, sent))\n",
    "\n",
    "        prev_key = current_key\n",
    "        lineList.clear()\n",
    "        lineList.append(line)\n",
    "        \n",
    "    if lineList:\n",
    "        max_score = -999\n",
    "        max_line = ''\n",
    "        for l in lineList:\n",
    "            ngm, sent = l.split('\\t')\n",
    "            word, col, dist = ngm.split()\n",
    "            score = computeScore(word, sent)\n",
    "            if score > max_score:\n",
    "                max_line = l\n",
    "                max_score = score\n",
    "        ngm, sent = max_line.split('\\t')\n",
    "        word, col, dist = ngm.split()\n",
    "#         print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
    "        reducerOutput.append('%s %s %s\\t%s' % (word, col, dist, sent))\n",
    "\n",
    "    print('---------- REDUCER END ----------')\n",
    "\n",
    "    return reducerOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeScore(word, sent):\n",
    "    global PRONS\n",
    "    global HiFreWords\n",
    "    \n",
    "    sent = sent.lower().split()\n",
    "    \n",
    "    locationOfWord = -1 if word not in sent else sent.index(word) \n",
    "    hiFreWordsScore = len([w for w in sent if w not in HiFreWords])\n",
    "    pronsScore = len([w for w in sent if w in PRONS])\n",
    "    \n",
    "    return locationOfWord - hiFreWordsScore - pronsScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- MAPPER START ----------\n",
      "MAPPER: Processing sentences ...  10%\n",
      "MAPPER: Processing sentences ...  20%\n",
      "MAPPER: Processing sentences ...  30%\n",
      "MAPPER: Processing sentences ...  40%\n",
      "MAPPER: Processing sentences ...  50%\n",
      "MAPPER: Processing sentences ...  60%\n",
      "MAPPER: Processing sentences ...  70%\n",
      "MAPPER: Processing sentences ...  80%\n",
      "MAPPER: Processing sentences ...  90%\n",
      "MAPPER: Processing sentences ... 100%\n",
      "MAPPER: Output length = 54038\n",
      "---------- MAPPER END ----------\n",
      "---------- REDUCER START ----------\n",
      "REDUCER: Start processing the best sentence (data size = 54038)\n",
      "---------- REDUCER END ----------\n",
      "CPU times: user 26.7 s, sys: 39.7 ms, total: 26.7 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reducerOutput = Reducer_testing(Mapper_testing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as outputFile:\n",
    "    for output in reducerOutput:\n",
    "        outputFile.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Local MapReduce testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mapper()\n",
    "Reducer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Saver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `lab07_good_dictionary_examples.py` exists. Overwrite (y/[N])?  y\n",
      "The following commands were written to file `lab07_good_dictionary_examples.py`:\n",
      "import re\n",
      "import sys\n",
      "from pprint import pprint\n",
      "from collections import defaultdict, Counter\n",
      "def read_collocation():\n",
      "    collocation = set()\n",
      "    for line in open('bnc.coll.small.txt'):\n",
      "        start, end, distance, count = line.split('\\t')\n",
      "        collocation.add(start + ' ' + end + ' ' + distance)\n",
      "    return collocation\n",
      "collocation = read_collocation()\n",
      "PRONS = set([line.strip('\\n') for line in open('prons.txt')])\n",
      "with open('HiFreWords') as f:\n",
      "    HiFreWords = set(f.readline().split('\\t'))\n",
      "print(len(collocation), len(PRONS), len(HiFreWords))\n",
      "def tokens(str1):\n",
      "    return re.findall('[a-z]+', str1.lower())\n",
      "\n",
      "def ngrams(sent, n):\n",
      "    return [' '.join(x) for x in zip(*[sent[i:] for i in range(n) if i <= len(sent)])]\n",
      "def isCollocation(start, end, d):\n",
      "    global collocation\n",
      "    if start + ' ' + end + ' ' + str(d) in collocation:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "def Mapper():\n",
      "    sentsNGrams = set()\n",
      "\n",
      "    for line in sys.stdin:\n",
      "        sent = tokens(line)\n",
      "        sentLen = len(sent)\n",
      "        \n",
      "        if sentLen < 10 or sentLen > 25:\n",
      "            continue\n",
      "            \n",
      "        for n in range(2, 6):\n",
      "            for ngram in ngrams(sent, n):\n",
      "                sentsNGrams.add(ngram)\n",
      "                \n",
      "        for ngram in sentsNGrams:\n",
      "            terms = ngram.split()\n",
      "            start = terms[0]\n",
      "            end = terms[-1]\n",
      "            n = len(terms)\n",
      "\n",
      "            if isCollocation(start, end, n-1):\n",
      "                print('%s %s %d\\t%s' % (start, end, n-1, line))\n",
      "            if isCollocation(end, start, 1-n):\n",
      "                print('%s %s %d\\t%s' % (end, start, 1-n, line))\n",
      "                \n",
      "        sentsNGrams.clear()\n",
      "def Mapper_testing():\n",
      "    print('---------- MAPPER START ----------')\n",
      "    counter = 0\n",
      "    sentsNGrams = set()\n",
      "    mapperOutput = []\n",
      "\n",
      "    for line in open('bnc.sents.txt'):\n",
      "        sent = tokens(line)\n",
      "        sentLen = len(sent)\n",
      "        \n",
      "        counter += 1\n",
      "        if counter % 20000 == 0:\n",
      "            print('MAPPER: Processing sentences ... %3d%%' % (counter / 2000))\n",
      "        \n",
      "        if sentLen < 10 or sentLen > 25:\n",
      "            continue\n",
      "            \n",
      "        for n in range(2, 6):\n",
      "            for ngram in ngrams(sent, n):\n",
      "                sentsNGrams.add(ngram)\n",
      "\n",
      "        for ngram in sentsNGrams:\n",
      "            terms = ngram.split()\n",
      "            start = terms[0]\n",
      "            end = terms[-1]\n",
      "            n = len(terms)\n",
      "\n",
      "            if isCollocation(start, end, n-1):\n",
      "                mapperOutput.append('%s %s %d\\t%s' % (start, end, n-1, line))\n",
      "            if isCollocation(end, start, 1-n):\n",
      "                mapperOutput.append('%s %s %d\\t%s' % (end, start, 1-n, line))\n",
      "                \n",
      "        sentsNGrams.clear()\n",
      "            \n",
      "    print('MAPPER: Output length = %d' % len(mapperOutput))\n",
      "    print('---------- MAPPER END ----------')\n",
      "\n",
      "    return sorted(mapperOutput)\n",
      "def Reducer():\n",
      "    lineList = []\n",
      "    prev_key = ''\n",
      "    isFirst = True\n",
      "    \n",
      "    for line in sys.stdin:\n",
      "        \n",
      "        if not isFirst:\n",
      "            current_key = line.split('\\t')[0]\n",
      "            if line.split('\\t')[0] == prev_key:\n",
      "                lineList.append(line)\n",
      "                prev_key = current_key\n",
      "                isFirst = False\n",
      "                continue\n",
      "        else:\n",
      "            isFirst = False\n",
      "            prev_key = line.split('\\t')[0]\n",
      "            lineList.append(line)\n",
      "            continue\n",
      "            \n",
      "        # All sentences in one group are read  \n",
      "        max_score = -999\n",
      "        max_line = ''\n",
      "        for l in lineList:\n",
      "            try:\n",
      "                ngm, sent = l.split('\\t')\n",
      "                word, col, dist = ngm.split()\n",
      "                score = computeScore(word, sent)\n",
      "                if score > max_score:\n",
      "                    max_line = l\n",
      "                    max_score = score\n",
      "            except:\n",
      "                pass\n",
      "        \n",
      "        ngm, sent = max_line.split('\\t')\n",
      "        word, col, dist = ngm.split()\n",
      "        print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "\n",
      "        prev_key = current_key\n",
      "        lineList.clear()\n",
      "        lineList.append(line)\n",
      "        \n",
      "    if lineList:\n",
      "        max_score = -999\n",
      "        max_line = ''\n",
      "        for l in lineList:\n",
      "            try:\n",
      "                ngm, sent = l.split('\\t')\n",
      "                word, col, dist = ngm.split()\n",
      "                score = computeScore(word, sent)\n",
      "                if score > max_score:\n",
      "                    max_line = l\n",
      "                    max_score = score\n",
      "            except:\n",
      "                pass\n",
      "    \n",
      "        ngm, sent = max_line.split('\\t')\n",
      "        word, col, dist = ngm.split()\n",
      "        print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "def Reducer_testing(mapperOutput):\n",
      "    print('---------- REDUCER START ----------')\n",
      "\n",
      "    lineList = []\n",
      "    reducerOutput = []\n",
      "    prev_key = ''\n",
      "    isFirst = True\n",
      "    counter = 0\n",
      "    mapperOutputCount = len(mapperOutput)\n",
      "    \n",
      "    print('REDUCER: Start processing the best sentence (data size = %d)' % mapperOutputCount)\n",
      "\n",
      "    for line in mapperOutput:\n",
      "        \n",
      "        counter += 1\n",
      "        if counter % (mapperOutputCount / 100) == 0:\n",
      "            print('REDUCER: Processing the best sentence ... %3d%%' % (counter * 100 / mapperOutputCount))\n",
      "        \n",
      "#         print('prev_key = %s' % prev_key)\n",
      "#         print('curr_key = %s' % line.split('\\t')[0])\n",
      "#         print(lineList)\n",
      "#         print('--------')\n",
      "        \n",
      "        if not isFirst:\n",
      "            current_key = line.split('\\t')[0]\n",
      "            if line.split('\\t')[0] == prev_key:\n",
      "                lineList.append(line)\n",
      "                prev_key = current_key\n",
      "                isFirst = False\n",
      "                continue\n",
      "        else:\n",
      "            isFirst = False\n",
      "            prev_key = line.split('\\t')[0]\n",
      "            lineList.append(line)\n",
      "            continue\n",
      "            \n",
      "        # All sentences in one group are read  \n",
      "        max_score = -999\n",
      "        max_line = ''\n",
      "        for l in lineList:\n",
      "            ngm, sent = l.split('\\t')\n",
      "            word, col, dist = ngm.split()\n",
      "            score = computeScore(word, sent)\n",
      "#             print(score, max_score, l)\n",
      "            if score > max_score:\n",
      "                max_line = l\n",
      "                max_score = score\n",
      "        \n",
      "        ngm, sent = max_line.split('\\t')\n",
      "        word, col, dist = ngm.split()\n",
      "#         print('BEST SENT:')\n",
      "#         print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "#         print()\n",
      "        reducerOutput.append('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "\n",
      "        prev_key = current_key\n",
      "        lineList.clear()\n",
      "        lineList.append(line)\n",
      "        \n",
      "    if lineList:\n",
      "        max_score = -999\n",
      "        max_line = ''\n",
      "        for l in lineList:\n",
      "            try:\n",
      "                ngm, sent = l.split('\\t')\n",
      "                word, col, dist = ngm.split()\n",
      "                score = computeScore(word, sent)\n",
      "                if score > max_score:\n",
      "                    max_line = l\n",
      "                    max_score = score\n",
      "            except:\n",
      "                pass\n",
      "        ngm, sent = max_line.split('\\t')\n",
      "        word, col, dist = ngm.split()\n",
      "#         print('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "        reducerOutput.append('%s %s %s\\t%s' % (word, col, dist, sent))\n",
      "\n",
      "    print('---------- REDUCER END ----------')\n",
      "\n",
      "    return reducerOutput\n",
      "def computeScore(word, sent):\n",
      "    global PRONS\n",
      "    global HiFreWords\n",
      "    \n",
      "    sent = sent.lower().split()\n",
      "    \n",
      "    locationOfWord = -1 if word not in sent else sent.index(word) \n",
      "    hiFreWordsScore = len([w for w in sent if w not in HiFreWords])\n",
      "    pronsScore = len([w for w in sent if w in PRONS])\n",
      "    \n",
      "    return locationOfWord - hiFreWordsScore - pronsScore\n",
      "get_ipython().run_cell_magic('time', '', 'reducerOutput = Reducer_testing(Mapper_testing())')\n",
      "with open('output.txt', 'w') as outputFile:\n",
      "    for output in reducerOutput:\n",
      "        outputFile.write(output)\n",
      "Mapper()\n",
      "Reducer()\n"
     ]
    }
   ],
   "source": [
    "%save lab07_good_dictionary_examples.py 123-136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
