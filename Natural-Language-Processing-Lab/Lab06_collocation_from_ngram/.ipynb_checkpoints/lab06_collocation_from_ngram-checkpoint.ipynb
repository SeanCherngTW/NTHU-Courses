{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter \n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from akl import akl\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate ngrams for a given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ngrams():\n",
    "    nGrams = defaultdict(int) \n",
    "    for line in open('citeseerx.ngms','r'):\n",
    "        ngram, count = line.split('\\t')\n",
    "        count = int(count)\n",
    "        nGrams[ngram] += count\n",
    "                \n",
    "    return nGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nGrams = read_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6715\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(nGrams['play-v a-det role-n'])\n",
    "print(nGrams['play-v a-det important-adj role-n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2-1: Generate skip bigrams from ngrams (-5 <= d <= 5) per 100 m. words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_skip_bigrams(nGrams):\n",
    "    \"\"\"\n",
    "    input  nGrams     : (nGram, count)\n",
    "    output skipBigrams: (skipBigram, position, count)\n",
    "    \"\"\"\n",
    "    skipBigrams = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for nGram, count in nGrams.items():\n",
    "        terms = nGram.split()\n",
    "        \n",
    "        start = terms[0]\n",
    "        end = terms[-1]\n",
    "        n = len(terms)\n",
    "        \n",
    "        key1 = '%s %s' % (start, end)\n",
    "        key2 = '%s %s' % (end, start)\n",
    "        \n",
    "        skipBigrams[key1][n-1] += count\n",
    "        skipBigrams[key2][-n+1] += count\n",
    "        \n",
    "    return skipBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skipBigrams = generate_skip_bigrams(nGrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2-2: Generate distance counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_distance_counts(skipBigrams):\n",
    "    dcSkipBigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
    "    \n",
    "    for nGram, distanceCounts in skipBigrams.items():\n",
    "        terms = nGram.split()\n",
    "        start = terms[0]\n",
    "        end = terms[1]\n",
    "        for distance, count in distanceCounts.items():\n",
    "            dcSkipBigrams[start][end][distance] += count\n",
    "            \n",
    "    return dcSkipBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dcSkipBigrams = generate_distance_counts(skipBigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collocation_extraction(word):\n",
    "    global nGrams\n",
    "    global dcSkipBigrams\n",
    "    \n",
    "    C1 = defaultdict(lambda: defaultdict(int))\n",
    "    final_skipBigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    final_nGrams = []\n",
    "\n",
    "    sum_freqi = 0\n",
    "    N = len(dcSkipBigrams[word].keys())\n",
    "    targetItem = dcSkipBigrams[word].items()\n",
    "    for end, distanceCounts in targetItem:\n",
    "        freqi = sum(distanceCounts.values())\n",
    "        sum_freqi += freqi\n",
    "        \n",
    "    avg_freq = sum_freqi / N\n",
    "    std_freq = 10e-6\n",
    "        \n",
    "    # Calculate standard deviation\n",
    "    for end, distanceCounts in targetItem:\n",
    "        freqi = sum(distanceCounts.values())\n",
    "        std_freq += math.sqrt((freqi - avg_freq) ** 2) / N\n",
    "\n",
    "    strength = 0.0\n",
    "        \n",
    "    # Condition 1\n",
    "    for end, distanceCounts in targetItem:\n",
    "        freqi = sum(distanceCounts.values())\n",
    "        strength = (freqi - avg_freq) / std_freq\n",
    "        if strength > 1:\n",
    "            C1[end] = distanceCounts\n",
    "\n",
    "    # Condition 2, 3\n",
    "    for end, distanceCounts in C1.items():\n",
    "        avg_pi = sum(distanceCounts.values()) / 10\n",
    "        Vi = 0.0\n",
    "        \n",
    "        # Condition 2\n",
    "        for distance, count in distanceCounts.items():\n",
    "            Vi += math.sqrt((count - avg_pi) ** 2) / 10\n",
    "            \n",
    "        if Vi <= 10:\n",
    "            continue\n",
    "        \n",
    "        # Condition 3\n",
    "        best_distance = 0\n",
    "        best_count = 0\n",
    "        for distance, count in distanceCounts.items():\n",
    "            threshold = avg_pi + math.sqrt(Vi)\n",
    "            if count > threshold:\n",
    "                if count > best_count:\n",
    "                    best_distance, best_count = distance, count\n",
    "            \n",
    "        final_skipBigrams[(end, best_distance)] = best_count\n",
    "        \n",
    "    # Sort in count\n",
    "    final_skipBigrams = sorted(final_skipBigrams.items(), key=operator.itemgetter(1), reverse=True)    \n",
    "                \n",
    "    # Get correponding nGram through skipBigrams\n",
    "    for distance, count in final_skipBigrams:\n",
    "        filter_nGrams = defaultdict(lambda: defaultdict(int))\n",
    "        collocation, length = distance\n",
    "        for k, v in nGrams.items():\n",
    "            terms = k.split()\n",
    "            start = terms[0]\n",
    "            end = terms[-1]\n",
    "            \n",
    "            if length > 0:\n",
    "                if start == word and end == collocation and len(terms) == length + 1:\n",
    "                    filter_nGrams[k] = v\n",
    "            else:\n",
    "                if start == collocation and end == word and len(terms) == 1 - length:\n",
    "                    filter_nGrams[k] = v\n",
    "        \n",
    "        final_nGrams.append(sorted(filter_nGrams.items(), key=operator.itemgetter(1), reverse=True)[0])\n",
    "                \n",
    "    return final_skipBigrams, final_nGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(word):\n",
    "    final_skipBigrams, final_nGrams = collocation_extraction(word)\n",
    "    print('Skip-Bigrams', 'Ngrams')\n",
    "    for a, b in zip(final_skipBigrams, final_nGrams):\n",
    "        print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Bigrams Ngrams\n",
      "(('in-prep', 1), 62342) ('role-n in-prep', 62342)\n",
      "(('play-v', -3), 41994) ('play-v an-det important-adj role-n', 15863)\n",
      "(('important-adj', -1), 23675) ('important-adj role-n', 23675)\n",
      "(('for-prep', 1), 8790) ('role-n for-prep', 8790)\n",
      "(('key-adj', -1), 5595) ('key-adj role-n', 5595)\n",
      "(('crucial-adj', -1), 4615) ('crucial-adj role-n', 4615)\n",
      "(('central-adj', -1), 4597) ('central-adj role-n', 4597)\n",
      "(('we-pron', -3), 3690) ('we-pron investigate-v the-det role-n', 892)\n",
      "(('critical-adj', -1), 3658) ('critical-adj role-n', 3658)\n",
      "(('major-adj', -1), 3257) ('major-adj role-n', 3257)\n",
      "(('on-prep', -2), 3242) ('on-prep the-det role-n', 3067)\n",
      "(('significant-adj', -1), 2994) ('significant-adj role-n', 2994)\n",
      "(('investigate-v', -2), 2683) ('investigate-v the-det role-n', 2612)\n",
      "(('have-v', -3), 2624) ('have-v an-det important-adj role-n', 686)\n",
      "(('by-prep', 2), 2400) ('role-n play-v by-prep', 2380)\n",
      "(('examine-v', -2), 2330) ('examine-v the-det role-n', 2301)\n",
      "(('as-prep', 1), 2296) ('role-n as-prep', 2296)\n",
      "(('be-v', 1), 2293) ('role-n be-v', 2293)\n",
      "(('essential-adj', -1), 2273) ('essential-adj role-n', 2273)\n",
      "(('vital-adj', -1), 1536) ('vital-adj role-n', 1536)\n",
      "(('fundamental-adj', -1), 1513) ('fundamental-adj role-n', 1513)\n",
      "(('potential-adj', -1), 1490) ('potential-adj role-n', 1490)\n",
      "(('discuss-v', -2), 1474) ('discuss-v the-det role-n', 1413)\n",
      "(('possible-adj', -1), 1444) ('possible-adj role-n', 1444)\n",
      "(('development-n', 3), 1381) ('role-n in-prep the-det development-n', 1232)\n",
      "(('determine-v', 2), 1310) ('role-n in-prep determine-v', 1310)\n",
      "(('functional-adj', -1), 1283) ('functional-adj role-n', 1283)\n",
      "(('many-adj', 2), 1249) ('role-n in-prep many-adj', 1188)\n",
      "(('pivotal-adj', -1), 1238) ('pivotal-adj role-n', 1238)\n",
      "(('paper-n', -3), 1222) ('paper-n examine-v the-det role-n', 288)\n",
      "(('semantic-adj', -1), 1219) ('semantic-adj role-n', 1219)\n",
      "(('explore-v', -2), 1107) ('explore-v the-det role-n', 1107)\n",
      "(('study-v', -2), 1098) ('study-v the-det role-n', 1098)\n",
      "(('understand-v', -2), 1018) ('understand-v the-det role-n', 959)\n",
      "(('about-prep', -2), 986) ('about-prep the-det role-n', 895)\n",
      "(('prominent-adj', -1), 880) ('prominent-adj role-n', 880)\n",
      "(('different-adj', -1), 831) ('different-adj role-n', 831)\n",
      "(('also-adv', -3), 827) ('also-adv play-v a-det role-n', 416)\n",
      "(('suggest-v', -2), 822) ('suggest-v a-det role-n', 776)\n",
      "(('very-adv', -2), 792) ('very-adv important-adj role-n', 669)\n",
      "(('increasingly-adv', -2), 782) ('increasingly-adv important-adj role-n', 669)\n",
      "(('with-prep', -2), 776) ('with-prep the-det role-n', 315)\n",
      "(('regulate-v', 2), 764) ('role-n in-prep regulate-v', 764)\n",
      "(('regulation-n', 3), 752) ('role-n in-prep the-det regulation-n', 698)\n",
      "(('study-n', -3), 624) ('study-n examine-v the-det role-n', 147)\n",
      "(('maintain-v', -3), 548) ('maintain-v this-det important-adj role-n', 548)\n",
      "(('support-v', -2), 464) ('support-v a-det role-n', 301)\n",
      "(('cell-n', 2), 426) ('role-n in-prep cell-n', 366)\n",
      "(('system-n', -4), 371) ('system-n play-v an-det important-adj role-n', 178)\n",
      "(('it-pron', -4), 350) ('it-pron play-v an-det important-adj role-n', 138)\n"
     ]
    }
   ],
   "source": [
    "main('role-n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `lab06_collocation_from_ngram.py`:\n",
      "from collections import defaultdict, Counter \n",
      "from operator import itemgetter\n",
      "from pprint import pprint\n",
      "from akl import akl\n",
      "import operator\n",
      "import math\n",
      "def read_ngrams():\n",
      "    nGrams = defaultdict(int) \n",
      "    for line in open('citeseerx.ngms','r'):\n",
      "        ngram, count = line.split('\\t')\n",
      "        count = int(count)\n",
      "        nGrams[ngram] += count\n",
      "                \n",
      "    return nGrams\n",
      "nGrams = read_ngrams()\n",
      "print(nGrams['play-v a-det role-n'])\n",
      "print(nGrams['play-v a-det important-adj role-n'])\n",
      "def generate_skip_bigrams(nGrams):\n",
      "    \"\"\"\n",
      "    input  nGrams     : (nGram, count)\n",
      "    output skipBigrams: (skipBigram, position, count)\n",
      "    \"\"\"\n",
      "    skipBigrams = defaultdict(lambda: defaultdict(int))\n",
      "    \n",
      "    for nGram, count in nGrams.items():\n",
      "        terms = nGram.split()\n",
      "        \n",
      "        start = terms[0]\n",
      "        end = terms[-1]\n",
      "        n = len(terms)\n",
      "        \n",
      "        key1 = '%s %s' % (start, end)\n",
      "        key2 = '%s %s' % (end, start)\n",
      "        \n",
      "        skipBigrams[key1][n-1] += count\n",
      "        skipBigrams[key2][-n+1] += count\n",
      "        \n",
      "    return skipBigrams\n",
      "skipBigrams = generate_skip_bigrams(nGrams)\n",
      "def generate_distance_counts(skipBigrams):\n",
      "    dcSkipBigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
      "    \n",
      "    for nGram, distanceCounts in skipBigrams.items():\n",
      "        terms = nGram.split()\n",
      "        start = terms[0]\n",
      "        end = terms[1]\n",
      "        for distance, count in distanceCounts.items():\n",
      "            dcSkipBigrams[start][end][distance] += count\n",
      "            \n",
      "    return dcSkipBigrams\n",
      "dcSkipBigrams = generate_distance_counts(skipBigrams)\n",
      "def collocation_extraction(word):\n",
      "    global nGrams\n",
      "    global dcSkipBigrams\n",
      "    \n",
      "    C1 = defaultdict(lambda: defaultdict(int))\n",
      "    final_skipBigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
      "    final_nGrams = []\n",
      "\n",
      "    sum_freqi = 0\n",
      "    N = len(dcSkipBigrams[word].keys())\n",
      "    targetItem = dcSkipBigrams[word].items()\n",
      "    for end, distanceCounts in targetItem:\n",
      "        freqi = sum(distanceCounts.values())\n",
      "        sum_freqi += freqi\n",
      "        \n",
      "    avg_freq = sum_freqi / N\n",
      "    std_freq = 10e-6\n",
      "        \n",
      "    # Calculate standard deviation\n",
      "    for end, distanceCounts in targetItem:\n",
      "        freqi = sum(distanceCounts.values())\n",
      "        std_freq += math.sqrt((freqi - avg_freq) ** 2) / N\n",
      "\n",
      "    strength = 0.0\n",
      "        \n",
      "    # Condition 1\n",
      "    for end, distanceCounts in targetItem:\n",
      "        freqi = sum(distanceCounts.values())\n",
      "        strength = (freqi - avg_freq) / std_freq\n",
      "        if strength > 1:\n",
      "            C1[end] = distanceCounts\n",
      "\n",
      "    # Condition 2, 3\n",
      "    for end, distanceCounts in C1.items():\n",
      "        avg_pi = sum(distanceCounts.values()) / 10\n",
      "        Vi = 0.0\n",
      "        \n",
      "        # Condition 2\n",
      "        for distance, count in distanceCounts.items():\n",
      "            Vi += math.sqrt((count - avg_pi) ** 2) / 10\n",
      "            \n",
      "        if Vi <= 10:\n",
      "            continue\n",
      "        \n",
      "        # Condition 3\n",
      "        best_distance = 0\n",
      "        best_count = 0\n",
      "        for distance, count in distanceCounts.items():\n",
      "            threshold = avg_pi + math.sqrt(Vi)\n",
      "            if count > threshold:\n",
      "                if count > best_count:\n",
      "                    best_distance, best_count = distance, count\n",
      "            \n",
      "        final_skipBigrams[(end, best_distance)] = best_count\n",
      "        \n",
      "    # Sort in count\n",
      "    final_skipBigrams = sorted(final_skipBigrams.items(), key=operator.itemgetter(1), reverse=True)    \n",
      "                \n",
      "    # Get correponding nGram through skipBigrams\n",
      "    for distance, count in final_skipBigrams:\n",
      "        filter_nGrams = defaultdict(lambda: defaultdict(int))\n",
      "        collocation, length = distance\n",
      "        for k, v in nGrams.items():\n",
      "            terms = k.split()\n",
      "            start = terms[0]\n",
      "            end = terms[-1]\n",
      "            \n",
      "            if length > 0:\n",
      "                if start == word and end == collocation and len(terms) == length + 1:\n",
      "                    filter_nGrams[k] = v\n",
      "            else:\n",
      "                if start == collocation and end == word and len(terms) == 1 - length:\n",
      "                    filter_nGrams[k] = v\n",
      "        \n",
      "        final_nGrams.append(sorted(filter_nGrams.items(), key=operator.itemgetter(1), reverse=True)[0])\n",
      "                \n",
      "    return final_skipBigrams, final_nGrams\n",
      "def main(word):\n",
      "    final_skipBigrams, final_nGrams = collocation_extraction(word)\n",
      "    print('Skip-Bigrams','Ngrams')\n",
      "    for a, b in zip(final_skipBigrams, final_nGrams):\n",
      "        print(a, b)\n",
      "main('role-n')\n"
     ]
    }
   ],
   "source": [
    "%save lab06_collocation_from_ngram.py 261-271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
