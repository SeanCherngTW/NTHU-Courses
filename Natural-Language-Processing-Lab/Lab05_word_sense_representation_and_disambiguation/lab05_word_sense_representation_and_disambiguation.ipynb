{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import time\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import DictionaryProbDist as D\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# documents\n",
    "wordnet_data = [line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training = 90% / testing = 10%\n",
    "def split_train_test(data):\n",
    "    random.shuffle(data)\n",
    "    split_point = len(data) * 9 // 10\n",
    "    train_set, test_set = data[:split_point], data[split_point:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(wordnet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordnet_features(sentence):\n",
    "    sentence = sentence.replace('||', ' ').replace('; ', ' ')\n",
    "    features = {}\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in stopwords:\n",
    "            features[word] = True\n",
    "            if word not in features:\n",
    "                features[word] = 1\n",
    "            else:\n",
    "                features[word] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_engineering(train_set):\n",
    "    org_word = []\n",
    "    label = []\n",
    "    features = []\n",
    "    candidates = []\n",
    "    for train in train_set:\n",
    "        c = []\n",
    "        org_word += [''.join(train[:][0])]\n",
    "        label += [''.join(train[:][1])]\n",
    "        features += [wordnet_features(''.join(train[:][2]))]\n",
    "        for candidate in eval(train[:][3]).values():\n",
    "            c += [candidate]\n",
    "        candidates += [c]\n",
    "    return org_word, label, features, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_org_word, train_label, train_features, train_candidates = feature_engineering(train_set)\n",
    "test_org_word, test_label, test_features, test_candidates = feature_engineering(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test all data with all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sk_training_all(train_features, train_label, test_features, test_label, test_candidates):\n",
    "    global sklearn_classifier_all\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    output_candidates = []\n",
    "    test_set = []\n",
    "    correct = 0\n",
    "    N = len(test_label)\n",
    "    \n",
    "    for X, y in zip(test_features, test_label):\n",
    "        test_set.append((X, y))\n",
    "        \n",
    "    for i in range(N):\n",
    "        output_candidates.clear()\n",
    "        \n",
    "        feature = test_features[i]\n",
    "        candidate = test_candidates[i]\n",
    "        label = test_label[i]\n",
    "        \n",
    "        prediction = sklearn_classifier_all.prob_classify(feature)._prob_dict\n",
    "        sorted_pred = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        for result, prob in sorted_pred:\n",
    "            if result in candidate:\n",
    "                output_candidates.append((result, prob))\n",
    "                \n",
    "        if not output_candidates: \n",
    "            continue\n",
    "            \n",
    "        top_output_candidate = sorted(output_candidates, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "        if top_output_candidate == label:\n",
    "            correct += 1\n",
    "\n",
    "    print('hand acc = %.4f' % (correct / N))\n",
    "    print('nltk acc = %.4f' % nltk.classify.accuracy(sklearn_classifier_all, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 56s, sys: 2.84 s, total: 6min 59s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_set = []\n",
    "for X, y in zip(train_features, train_label):\n",
    "    train_set.append((X, y))\n",
    "    \n",
    "sklearn_classifier_all = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n",
      "hand acc = 0.6599\n",
      "nltk acc = 0.5043\n"
     ]
    }
   ],
   "source": [
    "sk_training_all(train_features, train_label, test_features, test_label, test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `lab05_word_sense_representation_and_disambiguation.py`:\n",
      "def sk_training_all(train_features, train_label, test_features, test_label, test_candidates):\n",
      "    global sklearn_classifier_all\n",
      "    print('== SkLearn MaxEnt ==')\n",
      "    output_candidates = []\n",
      "    test_set = []\n",
      "    correct = 0\n",
      "    N = len(test_label)\n",
      "    \n",
      "    for X, y in zip(test_features, test_label):\n",
      "        test_set.append((X, y))\n",
      "        \n",
      "    for i in range(N):\n",
      "        output_candidates.clear()\n",
      "        \n",
      "        feature = test_features[i]\n",
      "        candidate = test_candidates[i]\n",
      "        label = test_label[i]\n",
      "        \n",
      "        prediction = sklearn_classifier_all.prob_classify(feature)._prob_dict\n",
      "        sorted_pred = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
      "        \n",
      "        for result, prob in sorted_pred:\n",
      "            if result in candidate:\n",
      "                output_candidates.append((result, prob))\n",
      "                \n",
      "        if not output_candidates: \n",
      "            continue\n",
      "            \n",
      "        top_output_candidate = sorted(output_candidates, key=lambda x: x[1], reverse=True)[0][0]\n",
      "        \n",
      "        if top_output_candidate == label:\n",
      "            correct += 1\n",
      "\n",
      "    print('hand acc = %.4f' % (correct / N))\n",
      "    print('nltk acc = %.4f' % nltk.classify.accuracy(sklearn_classifier_all, test_set))\n",
      "get_ipython().run_cell_magic('time', '', 'train_set = []\\nfor X, y in zip(train_features, train_label):\\n    train_set.append((X, y))\\n    \\nsklearn_classifier_all = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)')\n",
      "sk_training_all(train_features, train_label, test_features, test_label, test_candidates)\n"
     ]
    }
   ],
   "source": [
    "%save lab05_word_sense_representation_and_disambiguation.py 10-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
