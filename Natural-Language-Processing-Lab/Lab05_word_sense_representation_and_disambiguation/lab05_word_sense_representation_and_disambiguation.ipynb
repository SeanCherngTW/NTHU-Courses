{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import time\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import DictionaryProbDist as D\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_data = [line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data):\n",
    "    random.shuffle(data)\n",
    "    split_point = len(data) * 9 // 10\n",
    "    train_set, test_set = data[:split_point], data[split_point:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(wordnet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordnet_features(sentence):\n",
    "    sentence = sentence.replace('||', ' ').replace('; ', ' ')\n",
    "    features = {}\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in stopwords:\n",
    "            if word not in features:\n",
    "                features[word] = 1\n",
    "            else:\n",
    "                features[word] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_engineering(train_set):\n",
    "    org_word = []\n",
    "    label = []\n",
    "    features = []\n",
    "    candidates = []\n",
    "    for train in train_set:\n",
    "        c = []\n",
    "        org_word += [''.join(train[:][0])]\n",
    "        label += [''.join(train[:][1])]\n",
    "        features += [wordnet_features(''.join(train[:][2]))]\n",
    "        for candidate in eval(train[:][3]).values():\n",
    "            c += [candidate]\n",
    "        candidates += [c]\n",
    "    return org_word, label, features, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_org_word, train_label, train_features, train_candidates = feature_engineering(train_set)\n",
    "test_org_word, test_label, test_features, test_candidates = feature_engineering(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_for_testing = ''.join(train_label[0])\n",
    "features_for_testing = train_features[0]\n",
    "candidates_for_testing = train_candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cognition.n.01', 'representation.n.02']\n"
     ]
    }
   ],
   "source": [
    "print(candidates_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sk_training_for_testing(train_features, train_label, test_features, test_label, test_candidates):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    final_result = []\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for X, y in zip(train_features, train_label):\n",
    "        train_set.append((X, y))\n",
    "        \n",
    "    for X, y in zip(test_features, test_label):\n",
    "        test_set.append((X, y))\n",
    "        \n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set[:10])\n",
    "    \n",
    "    for feature in test_features[:1]:\n",
    "        prediction = sklearn_classifier.prob_classify(features_for_testing)._prob_dict\n",
    "        pprint(sorted(prediction.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        for label, prob in sorted(prediction.items(), key=operator.itemgetter(1), reverse=True):\n",
    "            if label in candidates_for_testing:\n",
    "                final_result.append(label)\n",
    "        \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: counterpart-n-1\n",
      "Answer : cognition.n.01\n",
      "\n",
      "== SkLearn MaxEnt ==\n",
      "[('cognition.n.01', 0.99995799601992952),\n",
      " ('lost.a.01', 6.5572203758608934e-06),\n",
      " ('fast.a.01', 6.1243954694721308e-06),\n",
      " ('constitute.v.01', 6.0927873943277612e-06),\n",
      " ('produce.v.02', 5.5419524422340959e-06),\n",
      " ('energetic.a.01', 5.309539213976842e-06),\n",
      " ('belief.n.01', 5.3024468754546519e-06),\n",
      " ('activity.n.01', 2.3634005482595415e-06),\n",
      " ('commerce.n.01', 2.3634005482595415e-06),\n",
      " ('stimulating.a.01', 2.3488372023547602e-06)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cognition.n.01']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing: counterpart-n-1\\nAnswer : cognition.n.01\\n\")\n",
    "sk_training_for_testing(train_features, train_label, test_features, test_label, test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sk_training(train_features, train_label, test_features, test_label, test_candidates):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    final_result = []\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in zip(train_features, train_label):\n",
    "        train_set.append((X, y))\n",
    "        \n",
    "    for X, y in zip(test_features, test_label):\n",
    "        test_set.append((X, y))\n",
    "\n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set[:1000])\n",
    "    \n",
    "    for feature, candidate in zip(test_features[:5], test_candidates[:5]):\n",
    "        prediction = sklearn_classifier.prob_classify(wordnet_features(''.join(feature)))._prob_dict\n",
    "        sorted_pred = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for label, prob in sorted_pred:\n",
    "            if label in candidate:\n",
    "                final_result.append(label)\n",
    "                has_ans = True\n",
    "                break\n",
    "#         final_result += sklearn_classifier.prob_classify(wordnet_features(''.join(feature)))._prob_dict\n",
    "        if not has_ans:\n",
    "            final_result.append(sorted_pred[0][0])\n",
    "        has_ans = False\n",
    "        total += 1\n",
    "        \n",
    "    print('ANS:')\n",
    "    print(test_label[:5])\n",
    "    print('final_result:')\n",
    "    print(final_result)\n",
    "    for i in range(len(final_result)):\n",
    "        if test_label[i] == final_result[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    print('correct = %d, total = %d' %(correct, total))\n",
    "    print(nltk.classify.accuracy(sklearn_classifier, test_set[:5]))\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-b9b161b3a95f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_org_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-ca293967f9ef>\u001b[0m in \u001b[0;36msk_training\u001b[0;34m(train_features, train_label, test_org_word, test_features, test_label, test_candidates)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_org_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_org_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mfinal_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mhas_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "final_r = sk_training(train_features, train_label, test_org_word, test_features, test_label, test_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sk_training_all(train_features, train_label, test_features, test_label, test_candidates):\n",
    "    print('== SkLearn MaxEnt ==')\n",
    "    final_result = []\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in zip(train_features, train_label):\n",
    "        train_set.append((X, y))\n",
    "        \n",
    "    for X, y in zip(test_features, test_label):\n",
    "        test_set.append((X, y))\n",
    "\n",
    "    sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train_set)\n",
    "    \n",
    "    for feature, candidate in zip(test_features, test_candidates):\n",
    "        prediction = sklearn_classifier.prob_classify(wordnet_features(''.join(feature)))._prob_dict\n",
    "        sorted_pred = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for label, prob in sorted_pred:\n",
    "            if label in candidate:\n",
    "                final_result.append(label)\n",
    "                has_ans = True\n",
    "                break\n",
    "        if not has_ans:\n",
    "            final_result.append(sorted_pred[0][0])\n",
    "        has_ans = False\n",
    "        total += 1\n",
    "        \n",
    "#     print('ANS:')\n",
    "#     print(test_label)\n",
    "#     print('final_result:')\n",
    "#     print(final_result)\n",
    "    for i in range(len(final_result)):\n",
    "        if test_label[i] == final_result[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    print('correct = %d, total = %d' %(correct, total))\n",
    "    print(nltk.classify.accuracy(sklearn_classifier, test_set))\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n",
      "correct = 825, total = 2320\n",
      "0.49267241379310345\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "final_r_all = sk_training_all(train_features, train_label, test_features, test_label, test_candidates)\n",
    "elapsed_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476.90998911857605"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `lab05_word_sense_representation_and_disambiguation.py`:\n",
      "from __future__ import division\n",
      "import nltk\n",
      "import random\n",
      "import re\n",
      "import string\n",
      "import operator\n",
      "import time\n",
      "from pprint import pprint\n",
      "from collections import defaultdict, Counter\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.probability import DictionaryProbDist as D\n",
      "from nltk.classify import SklearnClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "with open('stopwords.txt', 'r') as f:\n",
      "    stopwords = [line.strip() for line in f]\n",
      "lmtzr = WordNetLemmatizer()\n",
      "def words(text):\n",
      "    return re.findall(r'\\w+', text.lower())\n",
      "wordnet_data = [line.strip().split('\\t') for line in open('wn.in.evp.cat.txt', 'r') if line.strip() != '']\n",
      "def split_train_test(data):\n",
      "    random.shuffle(data)\n",
      "    split_point = len(data) * 9 // 10\n",
      "    train_set, test_set = data[:split_point], data[split_point:]\n",
      "    return train_set, test_set\n",
      "train_set, test_set = split_train_test(wordnet_data)\n",
      "def wordnet_features(sentence):\n",
      "    sentence = sentence.replace('||', ' ').replace('; ', ' ')\n",
      "    features = {}\n",
      "    for word in sentence.lower().split():\n",
      "        if word not in stopwords:\n",
      "            if word not in features:\n",
      "                features[word] = 1\n",
      "            else:\n",
      "                features[word] += 1\n",
      "    return features\n",
      "def feature_engineering(train_set):\n",
      "    org_word = []\n",
      "    label = []\n",
      "    features = []\n",
      "    candidates = []\n",
      "    for train in train_set:\n",
      "        c = []\n",
      "        org_word += [''.join(train[:][0])]\n",
      "        label += [''.join(train[:][1])]\n",
      "        features += [wordnet_features(''.join(train[:][2]))]\n",
      "        for candidate in eval(train[:][3]).values():\n",
      "            c += [candidate]\n",
      "        candidates += [c]\n",
      "    return org_word, label, features, candidates\n",
      "train_org_word, train_label, train_features, train_candidates = feature_engineering(train_set)\n",
      "test_org_word, test_label, test_features, test_candidates = feature_engineering(test_set)\n",
      "label_for_testing = ''.join(train_label[0])\n",
      "features_for_testing = train_features[0]\n",
      "candidates_for_testing = train_candidates[0]\n"
     ]
    }
   ],
   "source": [
    "%save lab05_word_sense_representation_and_disambiguation.py 171-180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
