{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_pattern_list(input_pat):\n",
    "    pattern = []\n",
    "    final = []\n",
    "    for i in input_pat:\n",
    "        if i != '':\n",
    "            pattern.append(i)\n",
    "        else:\n",
    "            final.append(pattern.copy())\n",
    "            pattern.clear()\n",
    "\n",
    "    # Last one\n",
    "    final.append(pattern)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "correct_pat = open('correct_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
    "correct_pat = create_sentence_pattern_list(correct_pat)\n",
    "wrong_pat = open('wrong_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
    "wrong_pat = create_sentence_pattern_list(wrong_pat)\n",
    "\n",
    "# Testing data\n",
    "test_pat = open('test_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
    "test_pat = create_sentence_pattern_list(test_pat)\n",
    "\n",
    "# Label of testing data\n",
    "test_label = []\n",
    "for line in open('ef_test.ref.txt', 'r'):\n",
    "    sent, verb, pattern = line.strip('\\n').split('\\t')\n",
    "    test_label.append('%s, %s' % (verb[1:-1], pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Noisy channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_pos(sent1, sent2):\n",
    "    lst = sent1.split()\n",
    "    sublst = sent2.split()\n",
    "    if len(lst) < len(sublst):\n",
    "        lst, sublst = sublst, lst\n",
    "    count = 0\n",
    "    n = len(sublst)\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(n):\n",
    "            if lst[i] == sublst[j]:\n",
    "                count += 1\n",
    "            if count == n:\n",
    "                return i - n + 1\n",
    "    return -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_noisy_channel():\n",
    "    noisy_channel = defaultdict(lambda: defaultdict(int))\n",
    "    for objectW, objectC in zip(wrong_pat, correct_pat):\n",
    "        sentW, sentC = objectW[0], objectC[0]\n",
    "\n",
    "        for c in objectC[1:]:\n",
    "            verbC, grammarC, patternC = c.split('\\t')\n",
    "\n",
    "            for w in objectW[1:]:\n",
    "                verbW, grammarW, patternW = w.split('\\t')\n",
    "                \n",
    "                # Same word but different grammar pattern -> Add to the nosiy channel\n",
    "                # Consider 2 words as the same word if their position index difference < 3\n",
    "                if verbC == verbW:\n",
    "                    if grammarC != grammarW and abs(pattern_pos(sentC, patternC) - pattern_pos(sentW, patternW)) < 3:\n",
    "                        noisy_channel[grammarW]['COUNT'] += 1\n",
    "                        noisy_channel[grammarW][grammarC] += 1\n",
    "                        break\n",
    "                        \n",
    "    return noisy_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_channel = build_noisy_channel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lexical language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexical_language_model():\n",
    "    language_model = defaultdict(lambda: defaultdict(int))\n",
    "    for objectW, objectC in zip(wrong_pat, correct_pat):\n",
    "        sentW, sentC = objectW[0], objectC[0]\n",
    "\n",
    "        for c in objectC[1:]:\n",
    "            verbC, grammarC, patternC = c.split('\\t')\n",
    "\n",
    "            for w in objectW[1:]:\n",
    "                verbW, grammarW, patternW = w.split('\\t')\n",
    "                # Same word but different grammar pattern -> Add to the nosiy channel\n",
    "                if verbC == verbW:\n",
    "                    if grammarC != grammarW and abs(pattern_pos(sentC, patternC) - pattern_pos(sentW, patternW)) < 3:\n",
    "                        language_model['%s, %s' % (verbC, grammarW)][grammarC] += 1\n",
    "                        language_model['%s, %s' % (verbC, grammarC)][grammarC] += 1\n",
    "                        language_model[verbC]['EXIST'] = 1\n",
    "                        break\n",
    "                        \n",
    "    return language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = build_lexical_language_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prediction function combining noisy channel and language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pedit(verb, w):\n",
    "    candidates = {}\n",
    "    key = '%s, %s' %(verb, w)\n",
    "    language_model_smooth = sum(language_model[key].values()) + len(language_model[key].values())\n",
    "    if key in language_model:\n",
    "        for candidate, count in language_model[key].items():\n",
    "            language_model_prob = (count + 1) / language_model_smooth\n",
    "            \n",
    "            noisy_channel_smooth = len(noisy_channel[w]) + noisy_channel[w]['COUNT']\n",
    "            noisy_channel_prob = (noisy_channel[w][candidate] + 1) / noisy_channel_smooth\n",
    "            \n",
    "            candidate_prob = language_model_prob * noisy_channel_prob\n",
    "            candidates[candidate] = candidate_prob\n",
    "            \n",
    "        best_candidate, prob = max(candidates.items(), key=operator.itemgetter(1))\n",
    "        return best_candidate, prob\n",
    "    else:\n",
    "        return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction():\n",
    "    hit = 0\n",
    "    index = 0\n",
    "    for objectT, labelT in zip(test_pat, test_label):\n",
    "        index += 1\n",
    "        sentT = objectT[0]\n",
    "        verbL, grammarL = labelT.split(',')\n",
    "        grammarL = grammarL.split('->')[0][2:-1]\n",
    "        \n",
    "        for t in objectT[1:]:\n",
    "            verbT, grammarT, patternT = t.split('\\t')\n",
    "            \n",
    "            if verbT != verbL:\n",
    "                continue\n",
    "            \n",
    "            if verbT in language_model and grammarT == grammarL:\n",
    "                best_candidate, prob = Pedit(verbT, grammarT)\n",
    "                prediction = '%s, (%s -> %s)' % (verbT, grammarT, best_candidate)\n",
    "                \n",
    "                if labelT == prediction:\n",
    "                    hit += 1\n",
    "                    print('%d Correct' % index)\n",
    "                else:\n",
    "                    print('%d Wrong' % index)\n",
    "                \n",
    "                print('Label: %s' % labelT)\n",
    "                print('Pred : %s' % prediction)\n",
    "                print('Prob : %.4f\\n' % prob)\n",
    "        \n",
    "    total = len(test_label)\n",
    "    print('hit = %d, total = %d, accuracy = %f' % (hit, total, hit / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Correct\n",
      "Label: APPLY, (V for n -> V to n)\n",
      "Pred : APPLY, (V for n -> V to n)\n",
      "Prob : 0.0155\n",
      "\n",
      "2 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6479\n",
      "\n",
      "3 Correct\n",
      "Label: EXPLAIN, (V n -> V to n)\n",
      "Pred : EXPLAIN, (V n -> V to n)\n",
      "Prob : 0.0677\n",
      "\n",
      "4 Correct\n",
      "Label: APPLY, (V n -> V for n)\n",
      "Pred : APPLY, (V n -> V for n)\n",
      "Prob : 0.2066\n",
      "\n",
      "5 Correct\n",
      "Label: APPLY, (V to n -> V for n)\n",
      "Pred : APPLY, (V to n -> V for n)\n",
      "Prob : 0.2840\n",
      "\n",
      "6 Correct\n",
      "Label: APPLY, (V n -> V for n)\n",
      "Pred : APPLY, (V n -> V for n)\n",
      "Prob : 0.2066\n",
      "\n",
      "7 Correct\n",
      "Label: APPLY, (V n -> V for n)\n",
      "Pred : APPLY, (V n -> V for n)\n",
      "Prob : 0.2066\n",
      "\n",
      "8 Correct\n",
      "Label: APPLY, (V to n -> V for n)\n",
      "Pred : APPLY, (V to n -> V for n)\n",
      "Prob : 0.2835\n",
      "\n",
      "9 Correct\n",
      "Label: APPLY, (V n -> V for n)\n",
      "Pred : APPLY, (V n -> V for n)\n",
      "Prob : 0.2066\n",
      "\n",
      "10 Correct\n",
      "Label: EXPLAIN, (V about n -> V n)\n",
      "Pred : EXPLAIN, (V about n -> V n)\n",
      "Prob : 0.3728\n",
      "\n",
      "11 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "12 Wrong\n",
      "Label: EXPLAIN, (V n that -> V to n)\n",
      "Pred : EXPLAIN, (V n that -> V n)\n",
      "Prob : 0.4925\n",
      "\n",
      "13 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "14 Correct\n",
      "Label: APPLY, (V n -> V for n)\n",
      "Pred : APPLY, (V n -> V for n)\n",
      "Prob : 0.2066\n",
      "\n",
      "15 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "16 Correct\n",
      "Label: APPLY, (V to n -> V for n)\n",
      "Pred : APPLY, (V to n -> V for n)\n",
      "Prob : 0.2835\n",
      "\n",
      "17 Correct\n",
      "Label: APPLY, (V to n -> V for n)\n",
      "Pred : APPLY, (V to n -> V for n)\n",
      "Prob : 0.2835\n",
      "\n",
      "18 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "19 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "20 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "21 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "22 Correct\n",
      "Label: DISCUSS, (V about n -> V n)\n",
      "Pred : DISCUSS, (V about n -> V n)\n",
      "Prob : 0.6449\n",
      "\n",
      "23 Correct\n",
      "Label: EXPLAIN, (V n -> V to n)\n",
      "Pred : EXPLAIN, (V n -> V to n)\n",
      "Prob : 0.0676\n",
      "\n",
      "24 Correct\n",
      "Label: APPLY, (V to n -> V for n)\n",
      "Pred : APPLY, (V to n -> V for n)\n",
      "Prob : 0.2835\n",
      "\n",
      "25 Correct\n",
      "Label: ANSWER, (V of n -> V n)\n",
      "Pred : ANSWER, (V of n -> V n)\n",
      "Prob : 0.3617\n",
      "\n",
      "26 Correct\n",
      "Label: ANSWER, (V to n -> V n)\n",
      "Pred : ANSWER, (V to n -> V n)\n",
      "Prob : 0.2413\n",
      "\n",
      "27 Correct\n",
      "Label: ANSWER, (V about n -> V n)\n",
      "Pred : ANSWER, (V about n -> V n)\n",
      "Prob : 0.7828\n",
      "\n",
      "28 Correct\n",
      "Label: ANSWER, (V about n -> V n)\n",
      "Pred : ANSWER, (V about n -> V n)\n",
      "Prob : 0.7828\n",
      "\n",
      "29 Correct\n",
      "Label: ANSWER, (V to n -> V n)\n",
      "Pred : ANSWER, (V to n -> V n)\n",
      "Prob : 0.2413\n",
      "\n",
      "hit = 28, total = 29, accuracy = 0.965517\n"
     ]
    }
   ],
   "source": [
    "correction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `lab08_automatic_recognition_of_verb_Patterns_for_correcting_writing_errors.py`:\n",
      "import operator\n",
      "from pprint import pprint\n",
      "from collections import defaultdict\n",
      "def create_sentence_pattern_list(input_pat):\n",
      "    pattern = []\n",
      "    final = []\n",
      "    for i in input_pat:\n",
      "        if i != '':\n",
      "            pattern.append(i)\n",
      "        else:\n",
      "            final.append(pattern.copy())\n",
      "            pattern.clear()\n",
      "\n",
      "    # Last one\n",
      "    final.append(pattern)\n",
      "    return final\n",
      "# Training data\n",
      "correct_pat = open('correct_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
      "correct_pat = create_sentence_pattern_list(correct_pat)\n",
      "wrong_pat = open('wrong_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
      "wrong_pat = create_sentence_pattern_list(wrong_pat)\n",
      "\n",
      "# Testing data\n",
      "test_pat = open('test_pattern.txt', 'r').read().strip('\\n').split('\\n')\n",
      "test_pat = create_sentence_pattern_list(test_pat)\n",
      "\n",
      "# Label of testing data\n",
      "test_label = []\n",
      "for line in open('ef_test.ref.txt', 'r'):\n",
      "    sent, verb, pattern = line.strip('\\n').split('\\t')\n",
      "    test_label.append('%s, %s' % (verb[1:-1], pattern))\n",
      "def pattern_pos(sent1, sent2):\n",
      "    lst = sent1.split()\n",
      "    sublst = sent2.split()\n",
      "    if len(lst) < len(sublst):\n",
      "        lst, sublst = sublst, lst\n",
      "    count = 0\n",
      "    n = len(sublst)\n",
      "    for i in range(len(lst)):\n",
      "        for j in range(n):\n",
      "            if lst[i] == sublst[j]:\n",
      "                count += 1\n",
      "            if count == n:\n",
      "                return i - n + 1\n",
      "    return -10\n",
      "def build_noisy_channel():\n",
      "    noisy_channel = defaultdict(lambda: defaultdict(int))\n",
      "    for objectW, objectC in zip(wrong_pat, correct_pat):\n",
      "        sentW, sentC = objectW[0], objectC[0]\n",
      "\n",
      "        for c in objectC[1:]:\n",
      "            verbC, grammarC, patternC = c.split('\\t')\n",
      "\n",
      "            for w in objectW[1:]:\n",
      "                verbW, grammarW, patternW = w.split('\\t')\n",
      "                \n",
      "                # Same word but different grammar pattern -> Add to the nosiy channel\n",
      "                # Consider 2 words as the same word if their position index difference < 3\n",
      "                if verbC == verbW:\n",
      "                    if grammarC != grammarW and abs(pattern_pos(sentC, patternC) - pattern_pos(sentW, patternW)) < 3:\n",
      "                        noisy_channel[grammarW]['COUNT'] += 1\n",
      "                        noisy_channel[grammarW][grammarC] += 1\n",
      "                        break\n",
      "                        \n",
      "    return noisy_channel\n",
      "noisy_channel = build_noisy_channel()\n",
      "def build_lexical_language_model():\n",
      "    language_model = defaultdict(lambda: defaultdict(int))\n",
      "    for objectW, objectC in zip(wrong_pat, correct_pat):\n",
      "        sentW, sentC = objectW[0], objectC[0]\n",
      "\n",
      "        for c in objectC[1:]:\n",
      "            verbC, grammarC, patternC = c.split('\\t')\n",
      "\n",
      "            for w in objectW[1:]:\n",
      "                verbW, grammarW, patternW = w.split('\\t')\n",
      "                # Same word but different grammar pattern -> Add to the nosiy channel\n",
      "                if verbC == verbW:\n",
      "                    if grammarC != grammarW and abs(pattern_pos(sentC, patternC) - pattern_pos(sentW, patternW)) < 3:\n",
      "                        language_model['%s, %s' % (verbC, grammarW)][grammarC] += 1\n",
      "                        language_model['%s, %s' % (verbC, grammarC)][grammarC] += 1\n",
      "                        language_model[verbC]['EXIST'] = 1\n",
      "                        break\n",
      "                        \n",
      "    return language_model\n",
      "language_model = build_lexical_language_model()\n",
      "def Pedit(verb, w):\n",
      "    candidates = {}\n",
      "    key = '%s, %s' %(verb, w)\n",
      "    language_model_smooth = sum(language_model[key].values()) + len(language_model[key].values())\n",
      "    if key in language_model:\n",
      "        for candidate, count in language_model[key].items():\n",
      "            language_model_prob = (count + 1) / language_model_smooth\n",
      "            \n",
      "            noisy_channel_smooth = len(noisy_channel[w]) + noisy_channel[w]['COUNT']\n",
      "            noisy_channel_prob = (noisy_channel[w][candidate] + 1) / noisy_channel_smooth\n",
      "            \n",
      "            candidate_prob = language_model_prob * noisy_channel_prob\n",
      "            candidates[candidate] = candidate_prob\n",
      "            \n",
      "        best_candidate, prob = max(candidates.items(), key=operator.itemgetter(1))\n",
      "        return best_candidate, prob\n",
      "    else:\n",
      "        return key\n",
      "def correction():\n",
      "    hit = 0\n",
      "    index = 0\n",
      "    for objectT, labelT in zip(test_pat, test_label):\n",
      "        index += 1\n",
      "        sentT = objectT[0]\n",
      "        verbL, grammarL = labelT.split(',')\n",
      "        grammarL = grammarL.split('->')[0][2:-1]\n",
      "        \n",
      "        for t in objectT[1:]:\n",
      "            verbT, grammarT, patternT = t.split('\\t')\n",
      "            \n",
      "            if verbT != verbL:\n",
      "                continue\n",
      "            \n",
      "            if verbT in language_model and grammarT == grammarL:\n",
      "                best_candidate, prob = Pedit(verbT, grammarT)\n",
      "                prediction = '%s, (%s -> %s)' % (verbT, grammarT, best_candidate)\n",
      "                \n",
      "                if labelT == prediction:\n",
      "                    hit += 1\n",
      "                    print('%d Correct' % index)\n",
      "                else:\n",
      "                    print('%d Wrong' % index)\n",
      "                \n",
      "                print('Label: %s' % labelT)\n",
      "                print('Pred : %s' % prediction)\n",
      "                print('Prob : %.4f\\n' % prob)\n",
      "        \n",
      "    total = len(test_label)\n",
      "    print('hit = %d, total = %d, accuracy = %f' % (hit, total, hit / total))\n",
      "correction()\n"
     ]
    }
   ],
   "source": [
    "%save lab08_automatic_recognition_of_verb_Patterns_for_correcting_writing_errors 301-311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
